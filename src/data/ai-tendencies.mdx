import {SidebarLink} from '@/components/Sidebar.js'
import {SectionRolePlay} from '@/app/sections.js'
import Fig from '@/components/Fig.js';

## AI Tendencies

The work focuses on three <SidebarLink content={<SectionRolePlay/>}>tendencies</SidebarLink> observed in generative AI, and particularly in Large Language Models tuned for use as chatbots or assistants, that lead to surprising, unexpected, or nonsensical outputs. The installation surfaces these tendencies through its improvised theatrical scenes allowing audiences to explore, experience, and reflect on them.

### Lying
<Fig
    src="tendencies-lying.png"
    width={1024}
    height={445}
>    
    *aka hallucination*   
</Fig>
Responses generated by LLMs are not always accurate or true, but will be presented as if they are, making it appear that the AI is imagining (or hallucinating) the response. In 2023 a [mathematical study](https://doi.org/10.3390/math11102320) suggested that “hallucinations may be an intrinsic property of GPT models”, and a [2022 paper](https://arxiv.org/abs/2109.07958) suggested that the larger a model, the more likely it was to repeat human misonceptions such as breaking a mirror bringing bad luck.


### Being servile or overly friendly
<Fig
    src="tendencies-servile.png"
    width={1024}
    height={445}
>
    *aka sycophancy*     
</Fig>
Responses generated by LLMs may include flattery of the user, or tend towards agreeing with their stated beliefs, even in the face of potential counter arguments or evidence.

Sycophancy may stem from the fine-tuning step of AI assistants - a [2024 paper](https://arxiv.org/abs/2310.13548) suggested that  “human feedback may also encourage model responses that match user beliefs over truthful ones”. Similarly to the repetition of misconceptions, larger models were shown to be more sycophantic in relation to users’ political questions in a [2022 study](https://arxiv.org/abs/2212.09251)

### Being Antagonistic
<Fig
    src="tendencies-antagonistic.png"
    width={1024}
    height={445}
>
    *Aka waluigi-ness.*    
</Fig>

An LLM may do the opposite of what’s asked, or rather generate a response counter to the one intended or expected from a prompt. The [Waluigi Effect](https://www.lesswrong.com/posts/D7PumeYTDPfBTp3i7/the-waluigi-effect-mega-post) describes this tendency and hypothesises some reasons for it, including that fine-tuning a model to exhibit certain characteristics (i.e. friendliness or helpfulness) makes it easier to elicit the opposite, and that a large amount of text that will be in the LLM’s training data contains protagonist-antagonist tropes, making it likely that the LLM will generate text in that style.