import {SectionTheDirector, SectionRolePlay, SectionAiWeirdness} from '@/app/sections.js'
import {SidebarLink} from '@/components/Sidebar.js'

## Voice Generation

The voice generation was the most computationally intensive part of the project as each script features 50 to 60 lines, including stage directions spoken by <SidebarLink content={<SectionTheDirector/>}>the Director</SidebarLink>. As such, a significant amount of the installation’s character comes from the masks’ voices, which lead a scene forward. These are obtained by generating audio files from single script lines using the text-prompted generative audio model [Bark](https://github.com/suno-ai/bark), and then pitching and speeding them up and down in real-time using Unreal Engine’s MetaSounds, as the scene is performed.

Although we used it for text-to-speech purposes, Bark is a general purpose audio model able to interpret a prompt and synthesise other sounds from it additional to voices, such as music, laughter, hesitation, etc. The model displays a tendency to filler words such as ‘umm’, ‘like’ or ‘uhh’, and also to veer off script adding contextually relevant or <SidebarLink content={<SectionRolePlay/>}>plausible sounding nonsense</SidebarLink>, including screaming. The model’s output felt closest to amateur improvisation, and added a layer of unpredictability and comedy.

### Speech Bubbles

Due to Bark’s unpredictable behaviour and tendency to add words, the generated audio files were passed through a speech-to-text model to obtain a definitive list of script lines. For this, we used [Faster Whisper Large V2](https://huggingface.co/Systran/faster-whisper-large-v2) which provided time-stamps for each spoken word, allowing us to include speech bubble for each mask’s line as they are spoken, helping audiences interpret the masks’ distorted voices. However, just as Bark can sometimes re-interpret text, whisper can mis-interpret voices. This lead to improvised acting that <SidebarLink content={<SectionAiWeirdness/>}>deviated significantly</SidebarLink> from the original script. 

### Translated Subtitles

The final step in the AI generation process is to translate the lines to enable subtitles for non-english speaking audiences. In the interest of speed, a small multi-language LLM was used, the 8B parameter, instruct-tuned version of [Llama 3.1](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct).

{/* Parsing the result was also much simpler than parsing the original script from Qwen as mostly they could be split by line. There were some occasions where it hallucinated additional lines, however and so some scripts needed re-processing. */}
